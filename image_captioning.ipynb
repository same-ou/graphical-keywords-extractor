{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "tFab7bOyS4Dh"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install langchain transformers torch torchvision pillow\n",
    "!pip install google-generativeai langchain-google-genai\n",
    "!pip install tiktoken verovio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "qDr0CKDITS4A"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "from PIL import Image\n",
    "from typing import Type\n",
    "from pydantic import BaseModel\n",
    "\n",
    "# Define the input schema for the tool\n",
    "class ImageCaptioningInput(BaseModel):\n",
    "    image_path: str  # Path to the image file\n",
    "\n",
    "# Define the custom tool\n",
    "class ImageCaptioningTool():\n",
    "    name: str = \"image_captioning\"\n",
    "    description: str = \"Generate a caption for an image using a pre-trained BLIP model.\"\n",
    "    args_schema: Type[BaseModel] = ImageCaptioningInput\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Set up device (CUDA if available, else CPU)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"Using device: {self.device}\")\n",
    "\n",
    "        # Load BLIP model and processor\n",
    "        self.processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "        self.model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(self.device)\n",
    "\n",
    "    def run(self, image_path: str) -> str:\n",
    "        \"\"\"\n",
    "        Generate a caption for an image given its file path.\n",
    "\n",
    "        Args:\n",
    "            image_path (str): Path to the image file.\n",
    "\n",
    "        Returns:\n",
    "            str: Generated caption for the image.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Load and process the image\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "            inputs = self.processor(image, return_tensors=\"pt\").to(self.device)\n",
    "            outputs = self.model.generate(**inputs)\n",
    "            caption = self.processor.decode(outputs[0], skip_special_tokens=True)\n",
    "            return caption\n",
    "        except Exception as e:\n",
    "            return f\"Error processing the image: {str(e)}\"\n",
    "\n",
    "    async def _arun(self, *args, **kwargs):\n",
    "        raise NotImplementedError(\"Async version not implemented for this tool.\")\n",
    "\n",
    "\n",
    "# Define the input schema for the tool\n",
    "class OCRInput(BaseModel):\n",
    "    image_path: str  # Path to the image file\n",
    "\n",
    "# Define the custom OCR tool\n",
    "class OCRTool():\n",
    "    name: str = \"ocr\"\n",
    "    description: str = \"Extract text from an image using the GOT-OCR2_0 model.\"\n",
    "    args_schema: Type[BaseModel] = OCRInput\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Set up device (CUDA if available, else CPU)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"Using device: {self.device}\")\n",
    "\n",
    "        # Load GOT-OCR2_0 model and tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained('ucaslcl/GOT-OCR2_0', trust_remote_code=True)\n",
    "        self.model = AutoModel.from_pretrained('ucaslcl/GOT-OCR2_0', trust_remote_code=True, low_cpu_mem_usage=True, device_map='cuda', use_safetensors=True, pad_token_id=self.tokenizer.eos_token_id)\n",
    "        self.model = self.model.eval().to(self.device)\n",
    "\n",
    "    def run(self, image_path: str) -> str:\n",
    "        \"\"\"\n",
    "        Extract text from an image using the GOT-OCR2_0 model.\n",
    "\n",
    "        Args:\n",
    "            image_path (str): Path to the image file.\n",
    "\n",
    "        Returns:\n",
    "            str: Extracted text from the image.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Input the image file to the model\n",
    "            res = self.model.chat(self.tokenizer, image_path, ocr_type='ocr')  # We use 'ocr' for plain text extraction\n",
    "            return res\n",
    "\n",
    "        except Exception as e:\n",
    "            return f\"Error processing the image: {str(e)}\"\n",
    "\n",
    "    async def _arun(self, *args, **kwargs):\n",
    "        raise NotImplementedError(\"Async version not implemented for this tool.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "W5ZA-0NhVN2m"
   },
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "import google.generativeai as genai\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.agents import Tool\n",
    "from langchain.agents import initialize_agent\n",
    "\n",
    "# Define the input schema for the chain (both tools need to be used together)\n",
    "class KeywordGenerationInput(BaseModel):\n",
    "    image_path: str  # Path to the image file\n",
    "\n",
    "# Define the prompt for keyword generation\n",
    "prompt_template = \"\"\"\n",
    "You are a powerful AI assistant skilled in generating keywords for search engines.\n",
    "Given an image caption and OCR-extracted text, your task is to generate a list of highly relevant keywords.\n",
    "Make sure the keywords are related to the content in the image and cover different aspects that might be searched for.\n",
    "Here's the image caption and the extracted text:\n",
    "\n",
    "Image Caption: {caption}\n",
    "OCR Extracted Text: {ocr_text}\n",
    "\n",
    "Please generate a list of keywords (comma-separated).\n",
    "\"\"\"\n",
    "\n",
    "# Create the prompt using the template\n",
    "prompt = PromptTemplate(input_variables=[\"caption\", \"ocr_text\"], template=prompt_template)\n",
    "\n",
    "# Create the LLMChain for keyword generation \n",
    "GOOGLE_API_KEY = \"\"\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", google_api_key=GOOGLE_API_KEY, max_output_tokens=100)\n",
    "chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "# Define a combined tool that runs the captioning and OCR tools and passes their results to Google Gemini\n",
    "class KeywordGenerationTool():\n",
    "    name: str = \"keyword_generation\"\n",
    "    description: str = \"Generate relevant keywords from an image using both image captioning and OCR text extraction.\"\n",
    "    args_schema: Type[BaseModel] = KeywordGenerationInput\n",
    "\n",
    "    def __init__(self, captioning_tool, ocr_tool, chain):\n",
    "        super().__init__()\n",
    "        self.captioning_tool = captioning_tool\n",
    "        self.ocr_tool = ocr_tool\n",
    "        self.chain = chain\n",
    "\n",
    "    def run(self, image_path: str) -> str:\n",
    "        \"\"\"\n",
    "        Run the image captioning tool, the OCR tool, and generate keywords via the LLM.\n",
    "        Args:\n",
    "            image_path (str): Path to the image file.\n",
    "        Returns:\n",
    "            str: Generated list of keywords.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Step 1: Generate the caption using the image captioning tool\n",
    "            caption = self.captioning_tool.run(image_path)\n",
    "            # print(f\"Caption: {caption}\")\n",
    "\n",
    "            # Step 2: Extract text using the OCR tool\n",
    "            ocr_text = self.ocr_tool.run(image_path)\n",
    "            # print(f\"OCR Text: {ocr_text}\")\n",
    "\n",
    "            # Step 3: Use Google Gemini (via LLMChain) to generate keywords based on both caption and OCR text\n",
    "            keywords = self.chain.run(caption=caption, ocr_text=ocr_text)\n",
    "            return [keyword.strip() for keyword in keywords.split(',')]\n",
    "        except Exception as e:\n",
    "            return f\"Error generating keywords: {str(e)}\"\n",
    "\n",
    "    async def _arun(self, *args, **kwargs):\n",
    "        raise NotImplementedError(\"Async version not implemented for this tool.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P1McraeHdOti",
    "outputId": "547a4129-231c-4c70-9d0e-94382866cc40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Create instances of the captioning tool and OCR tool\n",
    "captioning_tool = ImageCaptioningTool()\n",
    "ocr_tool = OCRTool()\n",
    "\n",
    "# Create an instance of the keyword generation tool with the chain\n",
    "keyword_tool = KeywordGenerationTool(captioning_tool, ocr_tool, chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zYpKLjFbe-Fz",
    "outputId": "8f509d96-e095-4103-c92c-4a9e0337f69c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caption: a diagram of a system with a number of codes\n",
      "OCR Text: uses 1..* ServiceOperator 1..* uses <<System>> uses 0.* <<CO>> ManagementApplication UserAgent 1..* 1..* 1..* controlslifecycleof 1..1 1..1 1..1 1..1 uses <<CO>> 1..1 <<CO>> controlslifecycle ServiceTemplateHandler SubscriberManager 1..1 1..1 1..1 controls controls controls 0..* 0..* 0..* <<IO>> <<IO>> <<IO>> ServiceTemplate Subscriber UserGroup <<IO>> SubscriptionContract uses uses 0..* 0..* 0..* 1..1 controls <<CO>> notifies <<CO>> SubscriptionRegistrar SubscriptionAgent 1..1 0..* 0..*\n",
      "Generated Keywords: system diagram, UML diagram, class diagram, software architecture, service operator, management application, user agent, service template handler, subscriber manager, subscription registrar, subscription agent, service template, subscriber, user group, subscription contract,  CO (control object), IO (input/output), controls lifecycle, uses relationship, cardinality, 1..*, 0..*, 1..1, software design, system design,  modeling,  architecture diagram,  component diagram,  relationship diagram,  \n"
     ]
    }
   ],
   "source": [
    "# Path to your image file\n",
    "image_path = \"/content/img.png\"\n",
    "\n",
    "# Generate keywords using the tool\n",
    "keywords = keyword_tool.run(image_path)\n",
    "print(f\"Generated Keywords: {keywords}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
